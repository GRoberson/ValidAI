#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üîå Testes Offline Completos - Sem Vertex AI

Este arquivo demonstra todos os testes que podem ser executados
completamente offline, sem necessidade de conex√£o com Vertex AI
ou outros servi√ßos externos.
"""

import json
import time
import sys
from pathlib import Path
from typing import Dict, List, Any

# Adicionar o diret√≥rio raiz ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Importar apenas componentes que n√£o dependem de servi√ßos externos
from rag_enhanced.testing import (
    TestDataGenerator,
    TestValidators,
    MockServices,
    MockFileSystem,
    ValidationResult
)


class TestesOfflineCompletos:
    """
    üß™ Suite completa de testes offline
    
    Todos estes testes funcionam sem conex√£o externa:
    - Valida√ß√£o de configura√ß√µes
    - Processamento de arquivos
    - An√°lise de c√≥digo
    - Gera√ß√£o de dados
    - Valida√ß√£o de estruturas
    - Simula√ß√£o de cen√°rios
    """
    
    def __init__(self):
        self.generator = TestDataGenerator()
        self.validators = TestValidators()
        self.mock_services = MockServices()
        self.mock_fs = MockFileSystem()
        self.resultados = []
    
    def executar_todos_testes_offline(self) -> Dict[str, Any]:
        """
        üöÄ Executa todos os testes offline dispon√≠veis
        
        Returns:
            Resultados consolidados de todos os testes
        """
        print("üîå Executando Testes Offline Completos")
        print("=" * 60)
        print("‚úÖ Nenhuma conex√£o externa necess√°ria!")
        print("‚úÖ Funciona sem Vertex AI, GCS ou internet")
        
        start_time = time.time()
        
        # Executar todas as categorias de teste
        resultados = {
            "1_validacao_configuracao": self.testar_validacao_configuracao(),
            "2_processamento_arquivos": self.testar_processamento_arquivos(),
            "3_analise_codigo": self.testar_analise_codigo(),
            "4_geracao_dados": self.testar_geracao_dados(),
            "5_validacao_estruturas": self.testar_validacao_estruturas(),
            "6_simulacao_cenarios": self.testar_simulacao_cenarios(),
            "7_sistema_arquivos": self.testar_sistema_arquivos(),
            "8_performance_local": self.testar_performance_local(),
            "9_tratamento_erros": self.testar_tratamento_erros(),
            "10_utilitarios": self.testar_utilitarios()
        }
        
        total_time = time.time() - start_time
        
        # Compilar estat√≠sticas
        stats = self._compilar_estatisticas(resultados, total_time)
        
        # Exibir resumo
        self._exibir_resumo(stats)
        
        return {
            "resultados": resultados,
            "estatisticas": stats,
            "executado_em": time.strftime("%Y-%m-%d %H:%M:%S"),
            "tempo_total": total_time
        }
    
    def testar_validacao_configuracao(self) -> Dict[str, Any]:
        """
        üîß Testa valida√ß√£o de configura√ß√µes
        
        Valida diferentes tipos de configura√ß√£o sem conex√£o externa.
        """
        print("\nüîß Testando Valida√ß√£o de Configura√ß√£o...")
        
        testes = []
        
        # Teste 1: Configura√ß√£o v√°lida
        config_valida = {
            "project_id": "test-project-123",
            "location": "us-central1",
            "bucket_name": "test-bucket-valid",
            "corpus_name": "test-corpus",
            "max_file_size_mb": 50,
            "timeout_seconds": 30,
            "retry_attempts": 3
        }
        
        resultado = self.validators.validate_config(config_valida)
        testes.append({
            "nome": "configuracao_valida",
            "sucesso": resultado.is_valid,
            "detalhes": resultado.to_dict()
        })
        
        # Teste 2: Configura√ß√£o inv√°lida
        config_invalida = {
            "project_id": "",  # Inv√°lido
            "location": "invalid-location",  # Inv√°lido
            "bucket_name": "INVALID_BUCKET_NAME",  # Inv√°lido
            "max_file_size_mb": -1,  # Inv√°lido
            "timeout_seconds": 1000  # Muito alto
        }
        
        resultado = self.validators.validate_config(config_invalida)
        testes.append({
            "nome": "configuracao_invalida",
            "sucesso": not resultado.is_valid,  # Deve falhar
            "detalhes": resultado.to_dict()
        })
        
        # Teste 3: Configura√ß√£o parcial
        config_parcial = {
            "project_id": "test-project",
            "location": "us-central1"
            # Campos opcionais ausentes
        }
        
        resultado = self.validators.validate_config(config_parcial)
        testes.append({
            "nome": "configuracao_parcial",
            "sucesso": resultado.is_valid,
            "detalhes": resultado.to_dict()
        })
        
        # Teste 4: Valida√ß√£o de extens√µes
        config_extensoes = {
            "project_id": "test-project",
            "location": "us-central1",
            "supported_extensions": [".py", ".js", ".md", ".txt", ".json"]
        }
        
        resultado = self.validators.validate_config(config_extensoes)
        testes.append({
            "nome": "configuracao_extensoes",
            "sucesso": resultado.is_valid,
            "detalhes": resultado.to_dict()
        })
        
        print(f"  ‚úÖ {len([t for t in testes if t['sucesso']])}/{len(testes)} testes passaram")
        
        return {
            "categoria": "validacao_configuracao",
            "testes": testes,
            "total": len(testes),
            "sucessos": len([t for t in testes if t['sucesso']]),
            "taxa_sucesso": len([t for t in testes if t['sucesso']]) / len(testes)
        }
    
    def testar_processamento_arquivos(self) -> Dict[str, Any]:
        """
        üìÑ Testa processamento de arquivos
        
        Simula processamento de diferentes tipos de arquivo.
        """
        print("\nüìÑ Testando Processamento de Arquivos...")
        
        testes = []
        
        # Gerar arquivos de teste
        arquivos_teste = self.generator.generate_test_files(count=5)
        
        for i, arquivo in enumerate(arquivos_teste):
            # Teste de valida√ß√£o de arquivo
            file_data = {
                "name": arquivo.name,
                "content": arquivo.content,
                "size": arquivo.size,
                "mime_type": f"text/x-{arquivo.language}" if arquivo.language else "text/plain"
            }
            
            resultado = self.validators.validate_file_data(file_data)
            
            testes.append({
                "nome": f"arquivo_{i}_{arquivo.language}",
                "sucesso": resultado.is_valid,
                "detalhes": {
                    "arquivo": arquivo.name,
                    "tamanho": arquivo.size,
                    "linguagem": arquivo.language,
                    "validacao": resultado.to_dict()
                }
            })
            
            # Simular processamento no mock filesystem
            self.mock_fs.create_file(f"/test/{arquivo.name}", arquivo.content)
        
        # Teste de listagem de arquivos
        arquivos_listados = self.mock_fs.list_files("/test", "*.py")
        testes.append({
            "nome": "listagem_arquivos_python",
            "sucesso": len(arquivos_listados) > 0,
            "detalhes": {"arquivos_encontrados": len(arquivos_listados)}
        })
        
        # Teste de leitura de arquivo
        if arquivos_teste:
            primeiro_arquivo = arquivos_teste[0]
            try:
                conteudo = self.mock_fs.read_file(f"/test/{primeiro_arquivo.name}")
                testes.append({
                    "nome": "leitura_arquivo",
                    "sucesso": conteudo == primeiro_arquivo.content,
                    "detalhes": {"tamanho_lido": len(conteudo)}
                })
            except Exception as e:
                testes.append({
                    "nome": "leitura_arquivo",
                    "sucesso": False,
                    "detalhes": {"erro": str(e)}
                })
        
        print(f"  ‚úÖ {len([t for t in testes if t['sucesso']])}/{len(testes)} testes passaram")
        
        return {
            "categoria": "processamento_arquivos",
            "testes": testes,
            "total": len(testes),
            "sucessos": len([t for t in testes if t['sucesso']]),
            "taxa_sucesso": len([t for t in testes if t['sucesso']]) / len(testes)
        }
    
    def testar_analise_codigo(self) -> Dict[str, Any]:
        """
        üîç Testa an√°lise de c√≥digo
        
        Analisa estruturas e padr√µes em c√≥digo gerado.
        """
        print("\nüîç Testando An√°lise de C√≥digo...")
        
        testes = []
        
        # Gerar c√≥digos de diferentes linguagens
        linguagens = ["python", "javascript", "java"]
        
        for linguagem in linguagens:
            # Gerar arquivo de c√≥digo
            arquivo_codigo = self.generator.generate_code_file(
                language=linguagem,
                complexity="medium"
            )
            
            # An√°lise b√°sica do c√≥digo
            linhas = arquivo_codigo.content.split('\n')
            linhas_nao_vazias = [l for l in linhas if l.strip()]
            
            # Detectar estruturas b√°sicas
            tem_funcoes = any('def ' in linha or 'function ' in linha or 'public ' in linha 
                            for linha in linhas_nao_vazias)
            tem_classes = any('class ' in linha for linha in linhas_nao_vazias)
            tem_comentarios = any(linha.strip().startswith(('#', '//', '/*', '/**')) 
                                for linha in linhas_nao_vazias)
            
            testes.append({
                "nome": f"analise_codigo_{linguagem}",
                "sucesso": tem_funcoes or tem_classes,  # Deve ter estruturas
                "detalhes": {
                    "linguagem": linguagem,
                    "linhas_total": len(linhas),
                    "linhas_codigo": len(linhas_nao_vazias),
                    "tem_funcoes": tem_funcoes,
                    "tem_classes": tem_classes,
                    "tem_comentarios": tem_comentarios,
                    "tamanho_kb": len(arquivo_codigo.content.encode()) / 1024
                }
            })
        
        # Teste de detec√ß√£o de padr√µes
        codigo_python = '''
class ProcessadorDados:
    def __init__(self, config):
        self.config = config
        
    def processar(self, dados):
        try:
            resultado = self._validar_dados(dados)
            return self._transformar_dados(resultado)
        except Exception as e:
            self._log_erro(e)
            raise
            
    def _validar_dados(self, dados):
        if not dados:
            raise ValueError("Dados vazios")
        return dados
        
    def _transformar_dados(self, dados):
        return [item.upper() for item in dados]
        
    def _log_erro(self, erro):
        print(f"Erro: {erro}")
'''
        
        # An√°lise de padr√µes
        tem_tratamento_erro = 'try:' in codigo_python and 'except' in codigo_python
        tem_metodos_privados = '_' in codigo_python
        tem_docstrings = '"""' in codigo_python or "'''" in codigo_python
        tem_type_hints = ':' in codigo_python and '->' in codigo_python
        
        testes.append({
            "nome": "deteccao_padroes_python",
            "sucesso": tem_tratamento_erro and tem_metodos_privados,
            "detalhes": {
                "tratamento_erro": tem_tratamento_erro,
                "metodos_privados": tem_metodos_privados,
                "docstrings": tem_docstrings,
                "type_hints": tem_type_hints
            }
        })
        
        print(f"  ‚úÖ {len([t for t in testes if t['sucesso']])}/{len(testes)} testes passaram")
        
        return {
            "categoria": "analise_codigo",
            "testes": testes,
            "total": len(testes),
            "sucessos": len([t for t in testes if t['sucesso']]),
            "taxa_sucesso": len([t for t in testes if t['sucesso']]) / len(testes)
        }
    
    def testar_geracao_dados(self) -> Dict[str, Any]:
        """
        üé≤ Testa gera√ß√£o de dados de teste
        
        Verifica se os geradores produzem dados v√°lidos e variados.
        """
        print("\nüé≤ Testando Gera√ß√£o de Dados...")
        
        testes = []
        
        # Teste 1: Gera√ß√£o de arquivos de c√≥digo
        arquivos_codigo = self.generator.generate_test_files(count=10)
        
        linguagens_geradas = set(arquivo.language for arquivo in arquivos_codigo)
        complexidades_geradas = set(arquivo.complexity for arquivo in arquivos_codigo)
        
        testes.append({
            "nome": "geracao_arquivos_codigo",
            "sucesso": len(arquivos_codigo) == 10 and len(linguagens_geradas) > 1,
            "detalhes": {
                "arquivos_gerados": len(arquivos_codigo),
                "linguagens": list(linguagens_geradas),
                "complexidades": list(complexidades_geradas),
                "tamanho_medio": sum(a.size for a in arquivos_codigo) / len(arquivos_codigo) if len(arquivos_codigo) > 0 else 0
            }
        })
        
        # Teste 2: Gera√ß√£o de documenta√ß√£o
        docs = self.generator.generate_documentation_files(count=5)
        
        tipos_doc = set(Path(doc.name).stem for doc in docs)
        
        testes.append({
            "nome": "geracao_documentacao",
            "sucesso": len(docs) == 5 and 'README' in str(tipos_doc),
            "detalhes": {
                "documentos_gerados": len(docs),
                "tipos": list(tipos_doc),
                "tamanho_total": sum(doc.size for doc in docs)
            }
        })
        
        # Teste 3: Gera√ß√£o de queries
        queries = self.generator.generate_query_dataset(count=20)
        
        categorias_query = set(query['category'] for query in queries)
        linguagens_query = set(query['language'] for query in queries)
        
        testes.append({
            "nome": "geracao_queries",
            "sucesso": len(queries) == 20 and len(categorias_query) > 2,
            "detalhes": {
                "queries_geradas": len(queries),
                "categorias": list(categorias_query),
                "linguagens": list(linguagens_query),
                "tamanho_medio": sum(len(q['text']) for q in queries) / len(queries) if len(queries) > 0 else 0
            }
        })
        
        # Teste 4: Gera√ß√£o de configura√ß√µes
        configs = self.generator.generate_config_files(count=3)
        
        tipos_config = [Path(config.name).suffix for config in configs]
        
        testes.append({
            "nome": "geracao_configuracoes",
            "sucesso": len(configs) == 3 and '.json' in tipos_config,
            "detalhes": {
                "configs_geradas": len(configs),
                "tipos": tipos_config,
                "tamanhos": [config.size for config in configs]
            }
        })
        
        # Teste 5: Gera√ß√£o de cen√°rios de erro
        cenarios_erro = self.generator.generate_error_scenarios()
        
        tipos_erro = set(cenario['error_type'] for cenario in cenarios_erro)
        
        testes.append({
            "nome": "geracao_cenarios_erro",
            "sucesso": len(cenarios_erro) > 3 and len(tipos_erro) > 2,
            "detalhes": {
                "cenarios_gerados": len(cenarios_erro),
                "tipos_erro": list(tipos_erro),
                "nomes": [c['name'] for c in cenarios_erro]
            }
        })
        
        print(f"  ‚úÖ {len([t for t in testes if t['sucesso']])}/{len(testes)} testes passaram")
        
        return {
            "categoria": "geracao_dados",
            "testes": testes,
            "total": len(testes),
            "sucessos": len([t for t in testes if t['sucesso']]),
            "taxa_sucesso": len([t for t in testes if t['sucesso']]) / len(testes)
        }
    
    def testar_validacao_estruturas(self) -> Dict[str, Any]:
        """
        ‚úÖ Testa valida√ß√£o de diferentes estruturas de dados
        
        Valida JSON, YAML, resultados de processamento, etc.
        """
        print("\n‚úÖ Testando Valida√ß√£o de Estruturas...")
        
        testes = []
        
        # Teste 1: Valida√ß√£o JSON
        json_valido = {
            "name": "test_project",
            "version": "1.0.0",
            "config": {
                "timeout": 30,
                "retries": 3
            }
        }
        
        resultado = self.validators.validate_json_structure(json_valido)
        testes.append({
            "nome": "validacao_json_valido",
            "sucesso": resultado.is_valid,
            "detalhes": resultado.to_dict()
        })
        
        # Teste 2: JSON inv√°lido (string malformada)
        json_string_invalido = '{"name": "test", "invalid": }'
        
        resultado = self.validators.validate_json_structure(json_string_invalido)
        testes.append({
            "nome": "validacao_json_invalido",
            "sucesso": not resultado.is_valid,  # Deve falhar
            "detalhes": resultado.to_dict()
        })
        
        # Teste 3: Valida√ß√£o de resultado de processamento
        resultado_processamento = {
            "status": "success",
            "timestamp": "2024-01-01T12:00:00Z",
            "data": {"files_processed": 10},
            "metrics": {
                "processing_time": 2.5,
                "success_rate": 0.95,
                "error_count": 1
            }
        }
        
        resultado = self.validators.validate_processing_result(resultado_processamento)
        testes.append({
            "nome": "validacao_resultado_processamento",
            "sucesso": resultado.is_valid,
            "detalhes": resultado.to_dict()
        })
        
        # Teste 4: Valida√ß√£o de resultado de query
        resultado_query = {
            "query": "Como implementar uma fun√ß√£o em Python?",
            "response": "Para implementar uma fun√ß√£o em Python, use a palavra-chave 'def'...",
            "timestamp": "2024-01-01T12:00:00Z",
            "sources": [
                {"name": "python_docs.md", "relevance": 0.9},
                {"name": "tutorial.py", "relevance": 0.7}
            ],
            "confidence": 0.85,
            "processing_time": 1.2
        }
        
        resultado = self.validators.validate_query_result(resultado_query)
        testes.append({
            "nome": "validacao_resultado_query",
            "sucesso": resultado.is_valid,
            "detalhes": resultado.to_dict()
        })
        
        # Teste 5: Valida√ß√£o de m√©tricas de performance
        metricas_performance = {
            "response_time": 1.5,
            "throughput": 100.0,
            "error_rate": 0.05,
            "cpu_usage": 45.0,
            "memory_usage": 60.0
        }
        
        resultado = self.validators.validate_performance_metrics(metricas_performance)
        testes.append({
            "nome": "validacao_metricas_performance",
            "sucesso": resultado.is_valid,
            "detalhes": resultado.to_dict()
        })
        
        # Teste 6: Valida√ß√£o de lote de resultados
        lote_resultados = [
            {"status": "success", "timestamp": "2024-01-01T12:00:00Z"},
            {"status": "success", "timestamp": "2024-01-01T12:01:00Z"},
            {"status": "error", "timestamp": "2024-01-01T12:02:00Z", "error_message": "Timeout"},
            {"status": "success", "timestamp": "2024-01-01T12:03:00Z"}
        ]
        
        resultado = self.validators.validate_batch_results(lote_resultados)
        testes.append({
            "nome": "validacao_lote_resultados",
            "sucesso": resultado.is_valid,
            "detalhes": resultado.to_dict()
        })
        
        print(f"  ‚úÖ {len([t for t in testes if t['sucesso']])}/{len(testes)} testes passaram")
        
        return {
            "categoria": "validacao_estruturas",
            "testes": testes,
            "total": len(testes),
            "sucessos": len([t for t in testes if t['sucesso']]),
            "taxa_sucesso": len([t for t in testes if t['sucesso']]) / len(testes)
        }
    
    def testar_simulacao_cenarios(self) -> Dict[str, Any]:
        """
        üé≠ Testa simula√ß√£o de diferentes cen√°rios com mocks
        
        Simula condi√ß√µes reais sem conex√µes externas.
        """
        print("\nüé≠ Testando Simula√ß√£o de Cen√°rios...")
        
        testes = []
        
        # Teste 1: Cen√°rio normal
        self.mock_services.setup_scenario("normal")
        
        try:
            # Testar opera√ß√µes b√°sicas
            bucket = self.mock_services.storage.create_bucket("test-bucket-normal")
            upload = self.mock_services.storage.upload_blob("test-bucket-normal", "test.txt", b"content")
            query = self.mock_services.vertex_ai.generate_content("test query")
            
            testes.append({
                "nome": "cenario_normal",
                "sucesso": all([bucket, upload, query]),
                "detalhes": {
                    "bucket_criado": bucket is not None,
                    "upload_sucesso": upload is not None,
                    "query_sucesso": query is not None
                }
            })
        except Exception as e:
            testes.append({
                "nome": "cenario_normal",
                "sucesso": False,
                "detalhes": {"erro": str(e)}
            })
        
        # Teste 2: Cen√°rio de alta lat√™ncia
        self.mock_services.setup_scenario("high_latency")
        
        start_time = time.time()
        try:
            query = self.mock_services.vertex_ai.generate_content("test query with latency")
            elapsed = time.time() - start_time
            
            testes.append({
                "nome": "cenario_alta_latencia",
                "sucesso": elapsed > 1.0 and query is not None,  # Deve demorar mais
                "detalhes": {
                    "tempo_resposta": elapsed,
                    "query_sucesso": query is not None
                }
            })
        except Exception as e:
            testes.append({
                "nome": "cenario_alta_latencia",
                "sucesso": False,
                "detalhes": {"erro": str(e)}
            })
        
        # Teste 3: Cen√°rio com problemas de rede
        self.mock_services.setup_scenario("network_issues")
        
        sucessos = 0
        falhas = 0
        
        for i in range(10):
            try:
                self.mock_services.storage.upload_blob("test-bucket", f"file_{i}.txt", b"test")
                sucessos += 1
            except Exception:
                falhas += 1
        
        testes.append({
            "nome": "cenario_problemas_rede",
            "sucesso": falhas > 0 and sucessos > 0,  # Deve ter algumas falhas
            "detalhes": {
                "sucessos": sucessos,
                "falhas": falhas,
                "taxa_falha": falhas / (sucessos + falhas)
            }
        })
        
        # Teste 4: Cen√°rio de rate limiting
        self.mock_services.setup_scenario("rate_limiting")
        
        sucessos_rl = 0
        falhas_rl = 0
        
        for i in range(15):  # Mais que o limite
            try:
                self.mock_services.storage.upload_blob("test-bucket", f"rl_file_{i}.txt", b"test")
                sucessos_rl += 1
            except Exception:
                falhas_rl += 1
        
        testes.append({
            "nome": "cenario_rate_limiting",
            "sucesso": falhas_rl > 5,  # Deve ter muitas falhas por rate limit
            "detalhes": {
                "sucessos": sucessos_rl,
                "falhas": falhas_rl,
                "rate_limit_ativado": falhas_rl > 5
            }
        })
        
        # Teste 5: Obter estat√≠sticas dos mocks
        stats = self.mock_services.get_comprehensive_stats()
        
        testes.append({
            "nome": "estatisticas_mocks",
            "sucesso": isinstance(stats, dict) and 'storage' in stats,
            "detalhes": stats
        })
        
        print(f"  ‚úÖ {len([t for t in testes if t['sucesso']])}/{len(testes)} testes passaram")
        
        return {
            "categoria": "simulacao_cenarios",
            "testes": testes,
            "total": len(testes),
            "sucessos": len([t for t in testes if t['sucesso']]),
            "taxa_sucesso": len([t for t in testes if t['sucesso']]) / len(testes)
        }
    
    def testar_sistema_arquivos(self) -> Dict[str, Any]:
        """
        üìÅ Testa opera√ß√µes do sistema de arquivos mock
        
        Verifica opera√ß√µes de arquivo sem tocar o filesystem real.
        """
        print("\nüìÅ Testando Sistema de Arquivos Mock...")
        
        testes = []
        
        # Teste 1: Criar e ler arquivo
        conteudo_teste = "Este √© um arquivo de teste\ncom m√∫ltiplas linhas\ne conte√∫do variado."
        
        try:
            self.mock_fs.create_file("/test/arquivo_teste.txt", conteudo_teste)
            conteudo_lido = self.mock_fs.read_file("/test/arquivo_teste.txt")
            
            testes.append({
                "nome": "criar_ler_arquivo",
                "sucesso": conteudo_lido == conteudo_teste,
                "detalhes": {
                    "tamanho_original": len(conteudo_teste),
                    "tamanho_lido": len(conteudo_lido),
                    "conteudo_igual": conteudo_lido == conteudo_teste
                }
            })
        except Exception as e:
            testes.append({
                "nome": "criar_ler_arquivo",
                "sucesso": False,
                "detalhes": {"erro": str(e)}
            })
        
        # Teste 2: Listar arquivos
        # Criar v√°rios arquivos
        arquivos_criados = []
        for i in range(5):
            nome_arquivo = f"/test/arquivo_{i}.py"
            self.mock_fs.create_file(nome_arquivo, f"# Arquivo Python {i}\nprint('Hello {i}')")
            arquivos_criados.append(nome_arquivo)
        
        arquivos_listados = self.mock_fs.list_files("/test", "*.py")
        
        testes.append({
            "nome": "listar_arquivos",
            "sucesso": len(arquivos_listados) == 5,
            "detalhes": {
                "arquivos_criados": len(arquivos_criados),
                "arquivos_listados": len(arquivos_listados),
                "arquivos": arquivos_listados
            }
        })
        
        # Teste 3: Verificar exist√™ncia de arquivo
        existe_arquivo_real = self.mock_fs.file_exists("/test/arquivo_teste.txt")
        existe_arquivo_inexistente = self.mock_fs.file_exists("/test/arquivo_inexistente.txt")
        
        testes.append({
            "nome": "verificar_existencia",
            "sucesso": existe_arquivo_real and not existe_arquivo_inexistente,
            "detalhes": {
                "arquivo_real_existe": existe_arquivo_real,
                "arquivo_inexistente_existe": existe_arquivo_inexistente
            }
        })
        
        # Teste 4: Obter informa√ß√µes do arquivo
        try:
            info_arquivo = self.mock_fs.get_file_info("/test/arquivo_teste.txt")
            
            testes.append({
                "nome": "informacoes_arquivo",
                "sucesso": "size" in info_arquivo and "created_at" in info_arquivo,
                "detalhes": info_arquivo
            })
        except Exception as e:
            testes.append({
                "nome": "informacoes_arquivo",
                "sucesso": False,
                "detalhes": {"erro": str(e)}
            })
        
        # Teste 5: Deletar arquivo
        try:
            deletado = self.mock_fs.delete_file("/test/arquivo_teste.txt")
            ainda_existe = self.mock_fs.file_exists("/test/arquivo_teste.txt")
            
            testes.append({
                "nome": "deletar_arquivo",
                "sucesso": deletado and not ainda_existe,
                "detalhes": {
                    "operacao_delete": deletado,
                    "arquivo_ainda_existe": ainda_existe
                }
            })
        except Exception as e:
            testes.append({
                "nome": "deletar_arquivo",
                "sucesso": False,
                "detalhes": {"erro": str(e)}
            })
        
        print(f"  ‚úÖ {len([t for t in testes if t['sucesso']])}/{len(testes)} testes passaram")
        
        return {
            "categoria": "sistema_arquivos",
            "testes": testes,
            "total": len(testes),
            "sucessos": len([t for t in testes if t['sucesso']]),
            "taxa_sucesso": len([t for t in testes if t['sucesso']]) / len(testes)
        }
    
    def testar_performance_local(self) -> Dict[str, Any]:
        """
        ‚ö° Testa performance de opera√ß√µes locais
        
        Mede tempos de execu√ß√£o de opera√ß√µes sem rede.
        """
        print("\n‚ö° Testando Performance Local...")
        
        testes = []
        
        # Teste 1: Performance de gera√ß√£o de dados
        start_time = time.time()
        arquivos = self.generator.generate_test_files(count=50)
        tempo_geracao = time.time() - start_time
        
        testes.append({
            "nome": "performance_geracao_dados",
            "sucesso": tempo_geracao < 5.0,  # Deve ser r√°pido
            "detalhes": {
                "arquivos_gerados": len(arquivos),
                "tempo_segundos": tempo_geracao,
                "arquivos_por_segundo": len(arquivos) / tempo_geracao if tempo_geracao > 0 else 0
            }
        })
        
        # Teste 2: Performance de valida√ß√£o
        configs_teste = [
            {"project_id": f"test-{i}", "location": "us-central1"} 
            for i in range(100)
        ]
        
        start_time = time.time()
        validacoes_ok = 0
        for config in configs_teste:
            resultado = self.validators.validate_config(config)
            if resultado.is_valid:
                validacoes_ok += 1
        tempo_validacao = time.time() - start_time
        
        testes.append({
            "nome": "performance_validacao",
            "sucesso": tempo_validacao < 2.0,  # Deve ser r√°pido
            "detalhes": {
                "configs_validadas": len(configs_teste),
                "validacoes_ok": validacoes_ok,
                "tempo_segundos": tempo_validacao,
                "validacoes_por_segundo": len(configs_teste) / tempo_validacao if tempo_validacao > 0 else 0
            }
        })
        
        # Teste 3: Performance de opera√ß√µes mock
        start_time = time.time()
        operacoes_ok = 0
        for i in range(100):
            try:
                self.mock_services.storage.upload_blob("perf-bucket", f"file_{i}.txt", b"test data")
                operacoes_ok += 1
            except Exception:
                pass
        tempo_mock = time.time() - start_time
        
        testes.append({
            "nome": "performance_mocks",
            "sucesso": tempo_mock < 3.0,  # Deve ser r√°pido
            "detalhes": {
                "operacoes_tentadas": 100,
                "operacoes_ok": operacoes_ok,
                "tempo_segundos": tempo_mock,
                "operacoes_por_segundo": 100 / tempo_mock if tempo_mock > 0 else 0
            }
        })
        
        # Teste 4: Performance de an√°lise de c√≥digo
        codigo_grande = self.generator.generate_code_file("python", "high")
        
        start_time = time.time()
        linhas = codigo_grande.content.split('\n')
        funcoes = [l for l in linhas if 'def ' in l]
        classes = [l for l in linhas if 'class ' in l]
        comentarios = [l for l in linhas if l.strip().startswith('#')]
        tempo_analise = time.time() - start_time
        
        testes.append({
            "nome": "performance_analise_codigo",
            "sucesso": tempo_analise < 1.0,  # Deve ser muito r√°pido
            "detalhes": {
                "linhas_analisadas": len(linhas),
                "funcoes_encontradas": len(funcoes),
                "classes_encontradas": len(classes),
                "comentarios_encontrados": len(comentarios),
                "tempo_segundos": tempo_analise,
                "linhas_por_segundo": len(linhas) / tempo_analise if tempo_analise > 0 else 0
            }
        })
        
        print(f"  ‚úÖ {len([t for t in testes if t['sucesso']])}/{len(testes)} testes passaram")
        
        return {
            "categoria": "performance_local",
            "testes": testes,
            "total": len(testes),
            "sucessos": len([t for t in testes if t['sucesso']]),
            "taxa_sucesso": len([t for t in testes if t['sucesso']]) / len(testes)
        }
    
    def testar_tratamento_erros(self) -> Dict[str, Any]:
        """
        ‚ö†Ô∏è Testa tratamento de erros e cen√°rios de falha
        
        Verifica se erros s√£o tratados adequadamente.
        """
        print("\n‚ö†Ô∏è Testando Tratamento de Erros...")
        
        testes = []
        
        # Teste 1: Erro de arquivo n√£o encontrado
        try:
            self.mock_fs.read_file("/arquivo/inexistente.txt")
            testes.append({
                "nome": "erro_arquivo_nao_encontrado",
                "sucesso": False,  # Deveria ter dado erro
                "detalhes": {"erro": "N√£o gerou exce√ß√£o esperada"}
            })
        except FileNotFoundError:
            testes.append({
                "nome": "erro_arquivo_nao_encontrado",
                "sucesso": True,  # Erro esperado
                "detalhes": {"erro_capturado": "FileNotFoundError"}
            })
        except Exception as e:
            testes.append({
                "nome": "erro_arquivo_nao_encontrado",
                "sucesso": True,  # Qualquer erro √© aceit√°vel
                "detalhes": {"erro_capturado": str(type(e).__name__)}
            })
        
        # Teste 2: Valida√ß√£o com dados inv√°lidos
        dados_invalidos = {
            "project_id": None,  # Inv√°lido
            "location": 123,     # Tipo errado
            "bucket_name": "",   # Vazio
        }
        
        resultado = self.validators.validate_config(dados_invalidos)
        testes.append({
            "nome": "validacao_dados_invalidos",
            "sucesso": not resultado.is_valid and len(resultado.errors) > 0,
            "detalhes": {
                "validacao_falhou": not resultado.is_valid,
                "erros_encontrados": len(resultado.errors),
                "erros": resultado.errors
            }
        })
        
        # Teste 3: Mock com alta taxa de falha
        self.mock_services.storage.set_failure_rate(0.9)  # 90% de falha
        
        falhas_capturadas = 0
        sucessos_inesperados = 0
        
        for i in range(20):
            try:
                self.mock_services.storage.upload_blob("test-bucket", f"fail_test_{i}.txt", b"test")
                sucessos_inesperados += 1
            except Exception:
                falhas_capturadas += 1
        
        testes.append({
            "nome": "mock_alta_taxa_falha",
            "sucesso": falhas_capturadas > sucessos_inesperados,
            "detalhes": {
                "falhas_capturadas": falhas_capturadas,
                "sucessos_inesperados": sucessos_inesperados,
                "taxa_falha_real": falhas_capturadas / 20
            }
        })
        
        # Resetar taxa de falha
        self.mock_services.storage.set_failure_rate(0.0)
        
        # Teste 4: JSON malformado
        json_malformado = '{"nome": "teste", "valor": }'
        
        resultado = self.validators.validate_json_structure(json_malformado)
        testes.append({
            "nome": "json_malformado",
            "sucesso": not resultado.is_valid,
            "detalhes": resultado.to_dict()
        })
        
        # Teste 5: Cen√°rios de erro pr√©-definidos
        cenarios_erro = self.generator.generate_error_scenarios()
        
        tipos_erro_esperados = ["NetworkError", "AuthenticationError", "ValidationError"]
        tipos_encontrados = [c['error_type'] for c in cenarios_erro]
        
        testes.append({
            "nome": "cenarios_erro_predefinidos",
            "sucesso": any(tipo in tipos_encontrados for tipo in tipos_erro_esperados),
            "detalhes": {
                "cenarios_gerados": len(cenarios_erro),
                "tipos_esperados": tipos_erro_esperados,
                "tipos_encontrados": tipos_encontrados
            }
        })
        
        print(f"  ‚úÖ {len([t for t in testes if t['sucesso']])}/{len(testes)} testes passaram")
        
        return {
            "categoria": "tratamento_erros",
            "testes": testes,
            "total": len(testes),
            "sucessos": len([t for t in testes if t['sucesso']]),
            "taxa_sucesso": len([t for t in testes if t['sucesso']]) / len(testes)
        }
    
    def testar_utilitarios(self) -> Dict[str, Any]:
        """
        üîß Testa fun√ß√µes utilit√°rias e helpers
        
        Verifica funcionalidades auxiliares do sistema.
        """
        print("\nüîß Testando Utilit√°rios...")
        
        testes = []
        
        # Teste 1: Gera√ß√£o de perfil de configura√ß√£o
        perfil = self.generator.generate_config_profile("test_profile")
        
        testes.append({
            "nome": "geracao_perfil_config",
            "sucesso": isinstance(perfil, dict) and "name" in perfil and "settings" in perfil,
            "detalhes": {
                "nome_perfil": perfil.get("name"),
                "tem_settings": "settings" in perfil,
                "campos_settings": list(perfil.get("settings", {}).keys())
            }
        })
        
        # Teste 2: Dados de performance
        dados_perf = self.generator.generate_performance_data()
        
        testes.append({
            "nome": "dados_performance",
            "sucesso": isinstance(dados_perf, dict) and "file_sizes" in dados_perf,
            "detalhes": {
                "campos": list(dados_perf.keys()),
                "tem_file_sizes": "file_sizes" in dados_perf,
                "tem_query_loads": "query_loads" in dados_perf
            }
        })
        
        # Teste 3: Cria√ß√£o de dados de teste pelos mocks
        dados_teste = self.mock_services.create_test_data(num_files=5)
        
        testes.append({
            "nome": "criacao_dados_teste_mock",
            "sucesso": len(dados_teste) == 5 and all(hasattr(f, 'name') for f in dados_teste),
            "detalhes": {
                "arquivos_criados": len(dados_teste),
                "nomes_arquivos": [f.name for f in dados_teste],
                "tamanhos": [f.size for f in dados_teste]
            }
        })
        
        # Teste 4: Estat√≠sticas dos mocks
        stats_antes = self.mock_services.get_comprehensive_stats()
        
        # Fazer algumas opera√ß√µes
        self.mock_services.storage.create_bucket("stats-test-bucket")
        self.mock_services.vertex_ai.create_corpus("stats-test-corpus", "Test")
        
        stats_depois = self.mock_services.get_comprehensive_stats()
        
        testes.append({
            "nome": "estatisticas_mocks_atualizadas",
            "sucesso": (stats_depois['storage']['buckets_count'] > stats_antes['storage']['buckets_count']),
            "detalhes": {
                "buckets_antes": stats_antes['storage']['buckets_count'],
                "buckets_depois": stats_depois['storage']['buckets_count'],
                "corpora_antes": stats_antes['vertex_ai']['corpora_count'],
                "corpora_depois": stats_depois['vertex_ai']['corpora_count']
            }
        })
        
        # Teste 5: Reset de mocks
        self.mock_services.reset_all_mocks()
        stats_reset = self.mock_services.get_comprehensive_stats()
        
        testes.append({
            "nome": "reset_mocks",
            "sucesso": stats_reset['storage']['buckets_count'] == 0,
            "detalhes": {
                "buckets_apos_reset": stats_reset['storage']['buckets_count'],
                "corpora_apos_reset": stats_reset['vertex_ai']['corpora_count']
            }
        })
        
        print(f"  ‚úÖ {len([t for t in testes if t['sucesso']])}/{len(testes)} testes passaram")
        
        return {
            "categoria": "utilitarios",
            "testes": testes,
            "total": len(testes),
            "sucessos": len([t for t in testes if t['sucesso']]),
            "taxa_sucesso": len([t for t in testes if t['sucesso']]) / len(testes)
        }
    
    def _compilar_estatisticas(self, resultados: Dict[str, Any], tempo_total: float) -> Dict[str, Any]:
        """Compila estat√≠sticas gerais dos testes"""
        total_testes = sum(categoria['total'] for categoria in resultados.values())
        total_sucessos = sum(categoria['sucessos'] for categoria in resultados.values())
        
        return {
            "total_categorias": len(resultados),
            "total_testes": total_testes,
            "total_sucessos": total_sucessos,
            "total_falhas": total_testes - total_sucessos,
            "taxa_sucesso_geral": total_sucessos / total_testes if total_testes > 0 else 0,
            "tempo_total_segundos": tempo_total,
            "tempo_medio_por_teste": tempo_total / total_testes if total_testes > 0 else 0,
            "categorias": {
                nome: {
                    "taxa_sucesso": categoria['taxa_sucesso'],
                    "testes": categoria['total']
                }
                for nome, categoria in resultados.items()
            }
        }
    
    def _exibir_resumo(self, stats: Dict[str, Any]) -> None:
        """Exibe resumo dos resultados"""
        print(f"\nüìä RESUMO FINAL DOS TESTES OFFLINE")
        print("=" * 60)
        print(f"üéØ Total de testes: {stats['total_testes']}")
        print(f"‚úÖ Sucessos: {stats['total_sucessos']}")
        print(f"‚ùå Falhas: {stats['total_falhas']}")
        print(f"üìà Taxa de sucesso: {stats['taxa_sucesso_geral']:.1%}")
        print(f"‚è±Ô∏è Tempo total: {stats['tempo_total_segundos']:.2f}s")
        print(f"‚ö° Tempo m√©dio por teste: {stats['tempo_medio_por_teste']:.3f}s")
        
        print(f"\nüìã Resultados por categoria:")
        for nome, categoria in stats['categorias'].items():
            status = "‚úÖ" if categoria['taxa_sucesso'] > 0.8 else "‚ö†Ô∏è" if categoria['taxa_sucesso'] > 0.5 else "‚ùå"
            print(f"  {status} {nome.replace('_', ' ').title()}: {categoria['taxa_sucesso']:.1%} ({categoria['testes']} testes)")
        
        if stats['taxa_sucesso_geral'] > 0.9:
            print(f"\nüéâ EXCELENTE! Todos os testes offline funcionando perfeitamente!")
        elif stats['taxa_sucesso_geral'] > 0.7:
            print(f"\nüëç BOM! A maioria dos testes offline est√° funcionando.")
        else:
            print(f"\n‚ö†Ô∏è ATEN√á√ÉO! Alguns testes offline precisam de corre√ß√£o.")


def main():
    """
    üöÄ Executa todos os testes offline
    """
    print("üîå TESTES OFFLINE COMPLETOS - RAG ENHANCED")
    print("=" * 60)
    print("üéØ Objetivo: Testar TUDO sem conex√£o externa")
    print("‚úÖ Sem Vertex AI, sem GCS, sem internet!")
    print("üß™ Framework de testes 100% local")
    
    # Executar testes
    tester = TestesOfflineCompletos()
    resultados_completos = tester.executar_todos_testes_offline()
    
    # Salvar relat√≥rio
    relatorio_path = Path("relatorio_testes_offline.json")
    with open(relatorio_path, 'w', encoding='utf-8') as f:
        json.dump(resultados_completos, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nüíæ Relat√≥rio completo salvo em: {relatorio_path}")
    
    # Exibir conclus√£o
    stats = resultados_completos['estatisticas']
    if stats['taxa_sucesso_geral'] > 0.8:
        print(f"\nüéâ SUCESSO! Framework de testes offline est√° funcionando perfeitamente!")
        print(f"üöÄ Pronto para desenvolvimento sem depend√™ncias externas!")
    else:
        print(f"\n‚ö†Ô∏è Alguns ajustes necess√°rios no framework de testes.")
    
    return resultados_completos


if __name__ == "__main__":
    main()